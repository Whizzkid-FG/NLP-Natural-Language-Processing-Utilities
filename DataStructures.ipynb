{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Whizzkid-FG/NLP-Natural-Language-Processing-Utilities/blob/main/DataStructures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOkcvB9sqy6"
      },
      "source": [
        "<font color=\"#00d2d3\">Build Vocabulary - aka Preprocessing Words into Cleaned Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVQ6RanQYPdp"
      },
      "source": [
        "#<font color='#4cd137'> Bag oF Words Architecture: <br>Tokenize - Create Vocab - Count Frequency of Tokens - Create Vectorizaed Representation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzCx_Vrwf46L"
      },
      "source": [
        "import pandas as pd\n",
        "from keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VE-9F3jpgFZF"
      },
      "source": [
        "text = [\n",
        "  'A rose is a flower',\n",
        "  'A fern is not a flower',\n",
        "  'the dog ate the rose',\n",
        "]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBAE2LB3E39a",
        "outputId": "351b88c8-73bc-46dd-c484-93e86f52552c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWtfOhYsjEBX"
      },
      "source": [
        "<font color='#4cd137'> Create a pandas series object from the list of sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUBN1gj7gFfv",
        "outputId": "7fea1780-55bd-4102-87b8-68141aee99a7"
      },
      "source": [
        "corpus_vocab = pd.Series(text)\n",
        "corpus_vocab"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        A rose is a flower\n",
              "1    A fern is not a flower\n",
              "2      the dog ate the rose\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQC-5EYpjtj_"
      },
      "source": [
        "[Keras Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer)<br> tf.keras.preprocessing.text.Tokenizer \n",
        "<br> method fit_on_text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHzcf-YSgJpt"
      },
      "source": [
        "model = Tokenizer()\n",
        "model.fit_on_texts(text)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRng1MUuoAr7"
      },
      "source": [
        "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
        "\n",
        "word_counts: A dictionary of words and their counts.\n",
        "word_docs: A dictionary of words and how many documents each appeared in.\n",
        "word_index: A dictionary of words and their uniquely assigned integers.\n",
        "document_count:An integer count of the total number of documents that were used to fit the Tokenizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwqqUi7Ygbiu",
        "outputId": "66dbaf4c-0f9c-4645-8a5a-a4f9c086b6e8"
      },
      "source": [
        "print(list(model.word_index))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'rose', 'is', 'flower', 'the', 'fern', 'not', 'dog', 'ate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sqD2CfZgJvO",
        "outputId": "6d8e2f38-81dc-4615-ce38-ff34519cc83d"
      },
      "source": [
        "vec_rep = model.texts_to_matrix(text, mode='count')\n",
        "vec_rep"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 2., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 2., 0., 1., 1., 0., 1., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 2., 0., 0., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IETZjEmWhL98"
      },
      "source": [
        "The length of the vector will always be equal to vocabulary size * len document"
      ]
    }
  ]
}